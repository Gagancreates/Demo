import com.google.cloud.storage.BlobInfo;
import com.google.cloud.storage.Storage;
import com.google.cloud.storage.StorageOptions;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.nio.channels.Channels;
import java.nio.file.Files;
import java.nio.file.Paths;

public class ParquetToGCS {

    public static void main(String[] args) throws Exception {
        // Define the schema
        String avroSchema = """
                {
                  "type": "record",
                  "name": "User",
                  "fields": [
                    {"name": "id", "type": "int"},
                    {"name": "name", "type": "string"},
                    {"name": "age", "type": "int"}
                  ]
                }
                """;
        Schema schema = new Schema.Parser().parse(avroSchema);

        // Write Parquet file locally
        File tempFile = File.createTempFile("output", ".parquet");
        try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(new org.apache.hadoop.fs.Path(tempFile.getAbsolutePath()))
                .withSchema(schema)
                .withCompressionCodec(CompressionCodecName.SNAPPY)
                .build()) {

            // Create data records
            GenericRecord user1 = new GenericData.Record(schema);
            user1.put("id", 1);
            user1.put("name", "Alice");
            user1.put("age", 25);

            GenericRecord user2 = new GenericData.Record(schema);
            user2.put("id", 2);
            user2.put("name", "Bob");
            user2.put("age", 30);

            writer.write(user1);
            writer.write(user2);
        }

        // Upload Parquet file to Google Cloud Storage
        String bucketName = "your-bucket-name";
        String objectName = "output.parquet";
        uploadToGCS(bucketName, objectName, tempFile.getAbsolutePath());

        System.out.println("File uploaded to GCS: gs://" + bucketName + "/" + objectName);
    }

    private static void uploadToGCS(String bucketName, String objectName, String filePath) throws Exception {
        Storage storage = StorageOptions.getDefaultInstance().getService();
        try (FileInputStream fis = new FileInputStream(filePath)) {
            storage.create(
                    BlobInfo.newBuilder(bucketName, objectName).build(),
                    Channels.newChannel(fis)
            );
        }
    }
}

import com.google.cloud.storage.Blob;
import com.google.cloud.storage.Storage;
import com.google.cloud.storage.StorageOptions;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.avro.AvroParquetReader;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.avro.generic.GenericRecord;

import java.io.File;

public class ReadParquetFromGCS {

    public static void main(String[] args) throws Exception {
        // Download Parquet file from GCS
        String bucketName = "your-bucket-name";
        String objectName = "output.parquet";
        File localFile = downloadFromGCS(bucketName, objectName);

        // Read Parquet file
        try (ParquetReader<GenericRecord> reader = AvroParquetReader.<GenericRecord>builder(new Path(localFile.getAbsolutePath())).build()) {
            GenericRecord record;
            while ((record = reader.read()) != null) {
                System.out.println(record);
            }
        }
    }

    private static File downloadFromGCS(String bucketName, String objectName) throws Exception {
        Storage storage = StorageOptions.getDefaultInstance().getService();
        Blob blob = storage.get(bucketName, objectName);
        File tempFile = File.createTempFile("downloaded", ".parquet");
        try (FileOutputStream fos = new FileOutputStream(tempFile)) {
            fos.getChannel().transferFrom(Channels.newChannel(blob.reader()), 0, Long.MAX_VALUE);
        }
        return tempFile;
    }
}
<dependencies>
    <!-- Apache Parquet -->
    <dependency>
        <groupId>org.apache.parquet</groupId>
        <artifactId>parquet-avro</artifactId>
        <version>1.13.0</version>
    </dependency>
    <!-- Google Cloud Storage SDK -->
    <dependency>
        <groupId>com.google.cloud</groupId>
        <artifactId>google-cloud-storage</artifactId>
        <version>2.27.0</version>
    </dependency>
    <!-- Avro for schema -->
    <dependency>
        <groupId>org.apache.avro</groupId>
        <artifactId>avro</artifactId>
        <version>1.11.1</version>
    </dependency>
</dependencies>
