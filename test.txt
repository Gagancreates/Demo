import com.google.cloud.storage.BlobId;
import com.google.cloud.storage.BlobInfo;
import com.google.cloud.storage.Storage;
import com.google.cloud.storage.StorageOptions;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetFileWriter;
import org.apache.parquet.io.OutputFile;
import org.apache.parquet.io.PositionOutputStream;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.nio.channels.Channels;

public class ParquetToGCS {

    // Custom OutputFile implementation
    static class ByteArrayOutputFile implements OutputFile {
        private final ByteArrayOutputStream outputStream = new ByteArrayOutputStream();

        @Override
        public PositionOutputStream create(long blockSizeHint) {
            return new PositionOutputStream() {
                private long position = 0;

                @Override
                public long getPos() {
                    return position;
                }

                @Override
                public void write(int b) {
                    outputStream.write(b);
                    position++;
                }

                @Override
                public void write(byte[] b, int off, int len) {
                    outputStream.write(b, off, len);
                    position += len;
                }
            };
        }

        @Override
        public PositionOutputStream createOrOverwrite(long blockSizeHint) {
            return create(blockSizeHint);
        }

        @Override
        public boolean supportsBlockSize() {
            return false;
        }

        @Override
        public long defaultBlockSize() {
            return 0;
        }

        public byte[] toByteArray() {
            return outputStream.toByteArray();
        }
    }

    public static void main(String[] args) throws Exception {
        // Define schema
        String schemaString = """
            {
                "type": "record",
                "name": "User",
                "fields": [
                    {"name": "name", "type": "string"},
                    {"name": "age", "type": "int"}
                ]
            }
        """;
        Schema schema = new Schema.Parser().parse(schemaString);

        // Create a generic record
        GenericRecord record1 = new GenericData.Record(schema);
        record1.put("name", "Alice");
        record1.put("age", 30);

        GenericRecord record2 = new GenericData.Record(schema);
        record2.put("name", "Bob");
        record2.put("age", 25);

        // Use custom OutputFile to write data
        ByteArrayOutputFile outputFile = new ByteArrayOutputFile();
        try (AvroParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(outputFile)
                .withSchema(schema)
                .withCompressionCodec(org.apache.parquet.hadoop.metadata.CompressionCodecName.SNAPPY)
                .withWriteMode(ParquetFileWriter.Mode.OVERWRITE)
                .build()) {
            writer.write(record1);
            writer.write(record2);
        }

        // Upload to GCS
        Storage storage = StorageOptions.getDefaultInstance().getService();
        BlobId blobId = BlobId.of("your-bucket-name", "path/to/file.parquet");
        BlobInfo blobInfo = BlobInfo.newBuilder(blobId).build();
        try (var channel = storage.writer(blobInfo)) {
            channel.write(ByteBuffer.wrap(outputFile.toByteArray()));
        }

        System.out.println("Parquet file uploaded to GCS!");
    }
}
