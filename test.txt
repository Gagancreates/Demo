import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.hadoop.ParquetReader;
import org.apache.parquet.hadoop.example.GroupReadSupport;
import org.apache.parquet.example.data.Group;

public class ReadParquetFromGCS {
    public static void main(String[] args) throws Exception {
        String gcsFilePath = "gs://your-bucket-name/path/to/file.parquet";

        // Configure Hadoop for Google Cloud Storage
        Configuration conf = new Configuration();
        conf.set("fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem");
        conf.set("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS");

        ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(gcsFilePath))
                .withConf(conf)
                .build();

        Group group;
        while ((group = reader.read()) != null) {
            System.out.println(group.toString());
        }
        reader.close();
    }
}

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.example.data.simple.SimpleGroupFactory;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.example.GroupWriteSupport;
import org.apache.parquet.schema.MessageType;
import org.apache.parquet.schema.Types;

public class WriteParquetToGCS {
    public static void main(String[] args) throws Exception {
        String gcsFilePath = "gs://your-bucket-name/path/to/output.parquet";

        // Define Parquet Schema
        MessageType schema = Types.buildMessage()
                .required(org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT32).named("id")
                .required(org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY).named("name")
                .named("ExampleSchema");

        Configuration conf = new Configuration();
        conf.set("fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem");
        conf.set("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS");

        GroupWriteSupport.setSchema(schema, conf);

        try (ParquetWriter<Group> writer = new ParquetWriter<>(
                new Path(gcsFilePath),
                new GroupWriteSupport(),
                ParquetWriter.DEFAULT_COMPRESSION_CODEC_NAME,
                ParquetWriter.DEFAULT_BLOCK_SIZE,
                ParquetWriter.DEFAULT_PAGE_SIZE,
                ParquetWriter.DEFAULT_DICTIONARY_PAGE_SIZE,
                ParquetWriter.DEFAULT_IS_DICTIONARY_ENABLED,
                ParquetWriter.DEFAULT_IS_VALIDATING_ENABLED,
                conf)) {

            SimpleGroupFactory factory = new SimpleGroupFactory(schema);

            Group group = factory.newGroup()
                    .append("id", 1)
                    .append("name", "Alice");

            writer.write(group);
        }
        System.out.println("Parquet file written to: " + gcsFilePath);
    }
}
<dependencies>
    <!-- Apache Parquet Dependencies -->
    <dependency>
        <groupId>org.apache.parquet</groupId>
        <artifactId>parquet-avro</artifactId>
        <version>1.13.1</version>
    </dependency>

    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.2</version>
    </dependency>

    <!-- Google Cloud Storage Connector -->
    <dependency>
        <groupId>com.google.cloud.bigdataoss</groupId>
        <artifactId>gcs-connector</artifactId>
        <version>hadoop3-2.2.15</version> <!-- Ensure this version matches your Hadoop version -->
    </dependency>

    <!-- Google Cloud Storage SDK -->
    <dependency>
        <groupId>com.google.cloud</groupId>
        <artifactId>google-cloud-storage</artifactId>
        <version>2.8.1</version>
    </dependency>
</dependencies>
